name: &name itn
lang: en        # e.g. 'ru', 'en'

# Pretrained Nemo Models
pretrained_model: models/itn.nemo

trainer:
  devices: 2 # the number of gpus, 0 for CPU
  num_nodes: 1
  max_epochs: 10  # the number of training epochs
  enable_checkpointing: false  # provided by exp_manager
  logger: false  # provided by exp_manager
  accumulate_grad_batches: 1 # accumulates grads every k batches
  gradient_clip_val: 0.0
  precision: 16 # Should be set to 16 for O1 and O2 to enable the AMP.
  accelerator: gpu
  strategy: ddp # ddp
  log_every_n_steps: 1  # Interval of logging.
  val_check_interval: 1.0  # Set to 0.25 to check 4 times per epoch, or an int for number of iterations
  resume_from_checkpoint: #/data/NeMo/nemo_train_data/exp/nemo_experiments_v7/training_distilko_v7_1.1/2024-05-31_17-09-47/checkpoints/training_distilko_v7_1.1--val_loss=0.0018-epoch=2-last.ckpt #/home/mshan/OpenSource/nemo-personaai/nemo_experiments_v5/training_distilko/2024-04-09_21-46-37/checkpoints/training_distilko--val_loss=0.0113-epoch=12-last.ckpt # /home/mshan/nemo_work/nemo_experiments/training/2023-08-09_18-20-27/checkpoints/training--val_loss=0.0262-epoch=1.ckpt
 # The path to a checkpoint file to continue the training, restores the whole state including the epoch, step, LR schedulers, apex, etc.

model:
  do_training: true
  label_map: ??? #/home/mshan/OpenSource/nemo-personaai/datasets/itn_v6_1.2.2/label_map.txt #/home/mshan/nemo_work/Thutmose_tagger_model/datasets_v2_5000M/label_map.txt # /home/mshan/nemo_work/Thutmose_tagger_model/ko_data_num7/label_map.txt #???  # path/.../label_map.txt
  semiotic_classes: ??? # /home/mshan/OpenSource/nemo-personaai/datasets/itn_v6_1.2.2/semiotic_classes.txt #/home/mshan/nemo_work/Thutmose_tagger_model/datasets_v2_5000M/semiotic_classes.txt # /home/mshan/nemo_work/Thutmose_tagger_model/datasets_num7/semiotic_classes.txt #???  # path/to/.../semiotic_classes.txt
  max_sequence_len: 128
  lang: ${lang}
  hidden_size: 768

  optim:
    name: adamw
    lr: 3e-5
    weight_decay: 0.1

    sched:
      name: WarmupAnnealing                      

      # pytorch lightning args
      monitor: val_loss
      reduce_on_plateau: false

      # scheduler config override
      warmup_ratio: 0.1
      last_epoch: -1

  language_model:
    pretrained_model_name: monologg/distilkobert #bert-base-multilingual-cased     # For ru, try DeepPavlov/rubert-base-cased | For de or multilingual, try bert-base-multilingual-cased
    lm_checkpoint: null
    config_file: null # json file, precedence over config
    config: null

  tokenizer:
    tokenizer_name: ${model.language_model.pretrained_model_name} # or sentencepiece
    vocab_file: null #/home/mshan/OpenSource/nemo-personaai/datasets/distilko/distilko_vocab.txt # path to vocab file
    tokenizer_model: null #/home/mshan/OpenSource/nemo-personaai/datasets/distilko/tokenizer_distilko.model # only used if tokenizer is sentencepiece
    special_tokens: null

exp_manager:
  exp_dir: nemo_experiments_v8 # where to store logs and checkpoints
  name: training_distilko_v8_1.2 # name of experiment
  create_tensorboard_logger: True
  create_checkpoint_callback: True
  checkpoint_callback_params:
    save_top_k: 10 #3
    monitor: "val_loss"
    mode: "min"
    always_save_nemo: True

tokenizer:
    tokenizer_name: ${model.transformer} # or sentencepiece
    vocab_file: /home/mshan/study/fast-api/projects/itn_serving_server/models/distilko_vocab.txt # path to vocab file
    tokenizer_model: null # only used if tokenizer is sentencepiece
    special_tokens: null

# Data
data:
  train_ds:
    data_path: ???  # provide the full path to the file
    batch_size: 512
    shuffle: true
    num_workers: 8
    pin_memory: true
    drop_last: false

  validation_ds:
    data_path: ???  # provide the full path to the file.
    batch_size: 512
    shuffle: false
    num_workers: 8
    pin_memory: true
    drop_last: false


# Inference
inference:
  from_file: null # Path to the raw text, no labels required. Each sentence on a separate line
  out_file: null # Path to the output file
  batch_size: 1 # batch size for inference.from_file

# Export
export_onnx:
  export_path: null
